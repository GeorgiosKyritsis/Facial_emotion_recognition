\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iii}{chapter*.2}}
\citation{barsoum2016training}
\citation{he2016identity}
\citation{huang2017densely}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vii}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{ix}{chapter*.5}}
\citation{vinciarelli2009social}
\citation{vural2008automated}
\citation{lucey2009automatically}
\citation{ekman1971constants}
\citation{ekman1978facial}
\citation{lecun1990handwritten}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\citation{nair2010rectified}
\citation{srivastava2014dropout}
\citation{ioffe2015batch}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Neural Network Schematic\relax }}{2}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neural_network}{{1.1}{2}{Neural Network Schematic\relax }{figure.caption.6}{}}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{szegedy2015going}
\citation{huang2017densely}
\citation{zagoruyko2016wide}
\citation{deng2009imagenet}
\citation{parkhi2015deep}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Convolutional Neural Network Schematic\relax }}{4}{figure.caption.7}}
\newlabel{fig:CNN}{{1.2}{4}{Convolutional Neural Network Schematic\relax }{figure.caption.7}{}}
\citation{berretti2010set}
\citation{zhang1999feature}
\citation{deniz2011face}
\citation{shan2009facial}
\citation{zhao2007dynamic}
\citation{wang2012facial}
\citation{tian2001recognizing}
\citation{lecun1990handwritten}
\citation{deng2009imagenet}
\citation{krizhevsky2012imagenet,szegedy2015going}
\citation{dhall2014emotion}
\citation{yu2015image}
\citation{zeiler2013stochastic}
\citation{mollahosseini2016going}
\citation{szegedy2015going}
\citation{barsoum2016training}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:related work}{{2}{5}{Related Work}{chapter.2}{}}
\citation{krizhevsky2012imagenet}
\citation{tang2013challenges}
\citation{barsoum2016training}
\citation{barsoum2016training}
\citation{barsoum2016training}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Datasets}{7}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:datasets}{{3}{7}{Datasets}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}FER+}{7}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces FER vs FER+ examples. Top labels are FER and bottom labels are FER+ (after majority voting). Figure taken from\nobreakspace  {}\cite  {barsoum2016training}.\relax }}{7}{figure.caption.8}}
\newlabel{fig:FERPLUS_FER}{{3.1}{7}{FER vs FER+ examples. Top labels are FER and bottom labels are FER+ (after majority voting). Figure taken from~\cite {barsoum2016training}.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Examples of the Fer+ dataset. Each row consists of faces of the same emotion, starting from top row: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.\relax }}{8}{figure.caption.9}}
\newlabel{fig:ferplus}{{3.2}{8}{Examples of the Fer+ dataset. Each row consists of faces of the same emotion, starting from top row: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.\relax }{figure.caption.9}{}}
\citation{rothe2015dex}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Extended Cohn-Kanade (CK+)}{9}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Examples of the CK+ dataset. From left to right: (a) Anger AU 4, 7, 17, 23, 24, (b) Contempt AU 14, 15, 24, (c) Disgust AU 9, 15, 17, 24, (d) Fear AU 1, 4, 7, 20, 25, (e) Happy AU 6, 12, 16, 25, (f) Sadness AU 1, 2, 4, 15, 17, (g) Surprise AU 1, 2, 5, 25, 27\relax }}{9}{figure.caption.10}}
\newlabel{fig:Cohn_kanade}{{3.3}{9}{Examples of the CK+ dataset. From left to right: (a) Anger AU 4, 7, 17, 23, 24, (b) Contempt AU 14, 15, 24, (c) Disgust AU 9, 15, 17, 24, (d) Fear AU 1, 4, 7, 20, 25, (e) Happy AU 6, 12, 16, 25, (f) Sadness AU 1, 2, 4, 15, 17, (g) Surprise AU 1, 2, 5, 25, 27\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}IMDB-WIKI 500+}{9}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Examples of the IMDB-WIKI dataset.\relax }}{9}{figure.caption.12}}
\newlabel{fig:wiki}{{3.4}{9}{Examples of the IMDB-WIKI dataset.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Number of faces per emotion for each dataset. NE: Neutral, HA: Happiness, SU: Surprise, SA: Sadness, AN: Anger, DI: Disgust, FE: Fear, CO: Contempt\relax }}{9}{table.caption.13}}
\newlabel{tab:datasets}{{3.2}{9}{Number of faces per emotion for each dataset. NE: Neutral, HA: Happiness, SU: Surprise, SA: Sadness, AN: Anger, DI: Disgust, FE: Fear, CO: Contempt\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Action unit codes with their description and their association with basic emotional states.\relax }}{10}{table.caption.11}}
\newlabel{tab:AU}{{3.1}{10}{Action unit codes with their description and their association with basic emotional states.\relax }{table.caption.11}{}}
\citation{krizhevsky2012imagenet}
\citation{deng2009imagenet}
\citation{hornik1989multilayer}
\citation{simonyan2014very}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Architectures}{11}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:architectures}{{4}{11}{Architectures}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}VGG}{11}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces AlexNet architecture\relax }}{11}{figure.caption.14}}
\newlabel{fig:Alex_net}{{4.1}{11}{AlexNet architecture\relax }{figure.caption.14}{}}
\citation{szegedy2015going}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces VGG16 architecture\relax }}{12}{figure.caption.15}}
\newlabel{fig:VGG16_imagenet}{{4.2}{12}{VGG16 architecture\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The inception module used in GoogLeNet.\relax }}{12}{figure.caption.16}}
\newlabel{fig:GoogLeNet}{{4.3}{12}{The inception module used in GoogLeNet.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}GoogLeNet/Inception}{12}{section.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Residual Networks}{12}{section.4.3}}
\citation{he2016deep}
\citation{he2016identity}
\citation{he2016identity}
\citation{huang2017densely}
\citation{huang2017densely}
\citation{huang2017densely}
\citation{zagoruyko2016wide}
\citation{krizhevsky2009learning}
\citation{netzer2011reading}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Normal CNN connections vs CNN with residual connections\relax }}{13}{figure.caption.17}}
\newlabel{fig:ResNet_1}{{4.4}{13}{Normal CNN connections vs CNN with residual connections\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}ResNet}{13}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}DenseNet}{13}{subsection.4.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces (a) original Residual Unit, (b) enhanced Residual Unit. Extracted from\nobreakspace  {}\cite  {he2016identity}\relax }}{14}{figure.caption.18}}
\newlabel{fig:ResNet}{{4.5}{14}{(a) original Residual Unit, (b) enhanced Residual Unit. Extracted from~\cite {he2016identity}\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A 5-layer dense block with a growth rate of $k = 4$. Each layer takes as input the feature maps from all previous layers. Extracted from\nobreakspace  {}\cite  {huang2017densely}\relax }}{14}{figure.caption.19}}
\newlabel{fig:DenseNet}{{4.6}{14}{A 5-layer dense block with a growth rate of $k = 4$. Each layer takes as input the feature maps from all previous layers. Extracted from~\cite {huang2017densely}\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Wide ResNet}{15}{subsection.4.3.3}}
\citation{donahue2014decaf}
\citation{razavian2014cnn}
\citation{sermanet2013overfeat}
\citation{yosinski2014transferable}
\citation{azizpour2016factors}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Transfer Learning}{17}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:transfer learning}{{5}{17}{Transfer Learning}{chapter.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Feature extraction from Convolutional Neural Network.\relax }}{18}{figure.caption.20}}
\newlabel{fig:transfer_learning}{{5.1}{18}{Feature extraction from Convolutional Neural Network.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Feature extraction from a pre-trained Convolutional Neural Network.\relax }}{18}{figure.caption.21}}
\newlabel{fig:VGG16_transfer_learning_feature_extractor}{{5.2}{18}{Feature extraction from a pre-trained Convolutional Neural Network.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Fine-tuning the pre-trained network.\relax }}{18}{figure.caption.22}}
\newlabel{fig:VGG16_transfer_learning_finetuning}{{5.3}{18}{Fine-tuning the pre-trained network.\relax }{figure.caption.22}{}}
\citation{deng2013new}
\citation{girshick2015fast}
\citation{collobert2008unified}
\citation{ramsundar2015massively}
\citation{caruanamultitask}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Multi-task learning}{19}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:multi-task learning}{{6}{19}{Multi-task learning}{chapter.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Hard parameter sharing for multi-task learning in deep neural networks\relax }}{19}{figure.caption.23}}
\newlabel{fig:multitask_learning_hard}{{6.1}{19}{Hard parameter sharing for multi-task learning in deep neural networks\relax }{figure.caption.23}{}}
\citation{duong2015low}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Hard parameter sharing}{20}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Soft parameter sharing}{20}{section.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Soft parameter sharing for multi-task learning in deep neural networks\relax }}{20}{figure.caption.24}}
\newlabel{fig:multitask_learning_soft}{{6.2}{20}{Soft parameter sharing for multi-task learning in deep neural networks\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Experiments \& Results}{21}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:experiments and results}{{7}{21}{Experiments \& Results}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}State-of-the-art Architecture}{21}{section.7.1}}
\citation{barsoum2016training}
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces State of the Art Architecture\relax }}{22}{figure.caption.25}}
\newlabel{fig:state_of_the_art}{{7.1}{22}{State of the Art Architecture\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}VGG-13}{23}{section.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}VGG-16}{23}{section.7.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}VGG-19}{23}{section.7.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces VGG Architectures\relax }}{24}{figure.caption.26}}
\newlabel{fig:vgg13_16_19}{{7.2}{24}{VGG Architectures\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}ResNet-18}{25}{section.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}ResNet-34}{25}{section.7.6}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}ResNet-50}{25}{section.7.7}}
\citation{Parkhi15}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Examples of conversion from grayscale to RGB images\relax }}{26}{figure.caption.27}}
\newlabel{fig:resnets.png}{{7.3}{26}{Examples of conversion from grayscale to RGB images\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}DenseNet}{26}{section.7.8}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}Wide ResNet}{26}{section.7.9}}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Transfer Learning}{26}{section.7.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Examples of conversion from grayscale to RGB images\relax }}{27}{figure.caption.29}}
\newlabel{fig:gray_rgb.pdf}{{7.4}{27}{Examples of conversion from grayscale to RGB images\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.1}Using pre-trained CNN features}{27}{subsection.7.10.1}}
\@writefile{toc}{\contentsline {subsubsection}{VGG16 pretrained on ImageNet}{27}{section*.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Illustration of the proposed method for transfer learning.\relax }}{27}{figure.caption.30}}
\newlabel{fig:SVM_fc6_fc7.pdf}{{7.5}{27}{Illustration of the proposed method for transfer learning.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Fine-tuning VGG16 architecture.\relax }}{28}{figure.caption.34}}
\newlabel{VGG16_Finetuning}{{7.6}{28}{Fine-tuning VGG16 architecture.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{ResNet50 and InceptionV3 pretrained on ImageNet}{28}{section*.31}}
\@writefile{toc}{\contentsline {subsubsection}{VGG16 pretrained for face recognition}{28}{section*.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.2}Fine-tuning}{28}{subsection.7.10.2}}
\@writefile{toc}{\contentsline {subsubsection}{VGG16 pretrained on ImageNet and for Face Recognition}{28}{section*.33}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Accuracies from Convolutional Neural Networks\relax }}{29}{table.caption.36}}
\newlabel{tab:cnn_accuracies}{{7.1}{29}{Accuracies from Convolutional Neural Networks\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Multi-task Learning}{29}{section.7.11}}
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Discussion on the Results}{29}{section.7.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Multi-task learning\relax }}{30}{figure.caption.35}}
\newlabel{fig:multitask_real_1}{{7.7}{30}{Multi-task learning\relax }{figure.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Accuracies from Transfer Learning\relax }}{31}{table.caption.37}}
\newlabel{tab:transfer_learning_accuracies}{{7.2}{31}{Accuracies from Transfer Learning\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Accuracy from Multi-task learning\relax }}{31}{table.caption.38}}
\newlabel{tab:multitask_learning}{{7.3}{31}{Accuracy from Multi-task learning\relax }{table.caption.38}{}}
\citation{baltruvsaitis2016openface}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Best Architecture}{33}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:best algorithm}{{8}{33}{Best Architecture}{chapter.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Final Algorithm.\relax }}{34}{figure.caption.39}}
\newlabel{fig:final_model}{{8.1}{34}{Final Algorithm.\relax }{figure.caption.39}{}}
\bibdata{chapters/references}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{35}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusion}{{9}{35}{Conclusion}{chapter.9}{}}
\bibcite{azizpour2016factors}{{1}{2016}{{Azizpour et~al.}}{{Azizpour, Razavian, Sullivan, Maki, and Carlsson}}}
\bibcite{baltruvsaitis2016openface}{{2}{2016}{{Baltru{\v {s}}aitis et~al.}}{{Baltru{\v {s}}aitis, Robinson, and Morency}}}
\bibcite{barsoum2016training}{{3}{2016}{{Barsoum et~al.}}{{Barsoum, Zhang, Ferrer, and Zhang}}}
\bibcite{berretti2010set}{{4}{2010}{{Berretti et~al.}}{{Berretti, Del~Bimbo, Pala, Amor, and Daoudi}}}
\bibcite{caruanamultitask}{{5}{}{{Caruana}}{{}}}
\bibcite{collobert2008unified}{{6}{2008}{{Collobert and Weston}}{{}}}
\bibcite{deng2009imagenet}{{7}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{deng2013new}{{8}{2013}{{Deng et~al.}}{{Deng, Hinton, and Kingsbury}}}
\bibcite{deniz2011face}{{9}{2011}{{D{\'e}niz et~al.}}{{D{\'e}niz, Bueno, Salido, and De~la Torre}}}
\bibcite{dhall2014emotion}{{10}{2014}{{Dhall et~al.}}{{Dhall, Goecke, Joshi, Sikka, and Gedeon}}}
\bibcite{donahue2014decaf}{{11}{2014}{{Donahue et~al.}}{{Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, and Darrell}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{37}{chapter*.40}}
\bibcite{duong2015low}{{12}{2015}{{Duong et~al.}}{{Duong, Cohn, Bird, and Cook}}}
\bibcite{ekman1971constants}{{13}{1971}{{Ekman and Friesen}}{{}}}
\bibcite{ekman1978facial}{{14}{1978}{{Ekman and Friesen}}{{}}}
\bibcite{girshick2015fast}{{15}{2015}{{Girshick}}{{}}}
\bibcite{he2016deep}{{16}{2016{}}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he2016identity}{{17}{2016{}}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{hornik1989multilayer}{{18}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, and White}}}
\bibcite{huang2017densely}{{19}{2017}{{Huang et~al.}}{{Huang, Liu, Weinberger, and van~der Maaten}}}
\bibcite{ioffe2015batch}{{20}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{kingma2014adam}{{21}{2014}{{Kingma and Ba}}{{}}}
\bibcite{krizhevsky2009learning}{{22}{2009}{{Krizhevsky and Hinton}}{{}}}
\bibcite{krizhevsky2012imagenet}{{23}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lecun1990handwritten}{{24}{1990}{{LeCun et~al.}}{{LeCun, Boser, Denker, Henderson, Howard, Hubbard, and Jackel}}}
\bibcite{lucey2009automatically}{{25}{2009}{{Lucey et~al.}}{{Lucey, Cohn, Lucey, Matthews, Sridharan, and Prkachin}}}
\bibcite{mollahosseini2016going}{{26}{2016}{{Mollahosseini et~al.}}{{Mollahosseini, Chan, and Mahoor}}}
\bibcite{nair2010rectified}{{27}{2010}{{Nair and Hinton}}{{}}}
\bibcite{netzer2011reading}{{28}{2011}{{Netzer et~al.}}{{Netzer, Wang, Coates, Bissacco, Wu, and Ng}}}
\bibcite{Parkhi15}{{29}{2015{}}{{Parkhi et~al.}}{{Parkhi, Vedaldi, and Zisserman}}}
\bibcite{parkhi2015deep}{{30}{2015{}}{{Parkhi et~al.}}{{Parkhi, Vedaldi, Zisserman, et~al.}}}
\bibcite{ramsundar2015massively}{{31}{2015}{{Ramsundar et~al.}}{{Ramsundar, Kearnes, Riley, Webster, Konerding, and Pande}}}
\bibcite{razavian2014cnn}{{32}{2014}{{Razavian et~al.}}{{Razavian, Azizpour, Sullivan, and Carlsson}}}
\bibcite{rothe2015dex}{{33}{2015}{{Rothe et~al.}}{{Rothe, Timofte, and Van~Gool}}}
\bibcite{sermanet2013overfeat}{{34}{2013}{{Sermanet et~al.}}{{Sermanet, Eigen, Zhang, Mathieu, Fergus, and LeCun}}}
\bibcite{shan2009facial}{{35}{2009}{{Shan et~al.}}{{Shan, Gong, and McOwan}}}
\bibcite{simonyan2014very}{{36}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{srivastava2014dropout}{{37}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{szegedy2015going}{{38}{2015}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich, et~al.}}}
\bibcite{tang2013challenges}{{39}{2013}{{Tang}}{{}}}
\bibcite{tian2001recognizing}{{40}{2001}{{Tian et~al.}}{{Tian, Kanade, and Cohn}}}
\bibcite{vinciarelli2009social}{{41}{2009}{{Vinciarelli et~al.}}{{Vinciarelli, Pantic, and Bourlard}}}
\bibcite{vural2008automated}{{42}{2008}{{Vural et~al.}}{{Vural, {\c {C}}etin, Er{\c {c}}il, Littlewort, Bartlett, and Movellan}}}
\bibcite{wang2012facial}{{43}{2012}{{Wang and Ying}}{{}}}
\bibcite{yosinski2014transferable}{{44}{2014}{{Yosinski et~al.}}{{Yosinski, Clune, Bengio, and Lipson}}}
\bibcite{yu2015image}{{45}{2015}{{Yu and Zhang}}{{}}}
\bibcite{zagoruyko2016wide}{{46}{2016}{{Zagoruyko and Komodakis}}{{}}}
\bibcite{zeiler2013stochastic}{{47}{2013}{{Zeiler and Fergus}}{{}}}
\bibcite{zhang1999feature}{{48}{1999}{{Zhang}}{{}}}
\bibcite{zhao2007dynamic}{{49}{2007}{{Zhao and Pietikainen}}{{}}}
